# Abstract
Due: 02/13/2022

In recent years, reinforcement learning has become an increasingly popular method to search and act upon non-parametric state spaces that were originally limited to humans. Progress in computing hardware and reinforcement learning techniques, particularly Q learning as an algorithm that can train an agent without a model of the environment, has led to successful applications of RL techniques across a wide range of fields originally exclusive to human actors. One of the more active fields featuring RL agents operating at or beyond the capabilities of human counterparts is the gaming space (i.e. board games, video games). Previous research has applied RL techniques to traditional games such as checkers, chess, and go, to more modern entertainment including Dota 2, Starcraft, and League of Legends. For modern video games, in the training phase, the RL agent either actively plays the game or watches a long history of game play.

For this final project, I'd like to explore this domain of work with the goal of applying such techniques to a game that has not been tested yet by reinforcement learning techniques, specifically online "io" multiplayer games (i.e. Shell Shockers, Slither.io). In addition to training on gameplay, one aspect that I am interested in is training an RL agent on not only the gameplay, but also the game setup. For instance, in modern video games, often times players are allowed to select characters or assets that give the player a set of advantages and disadvantages. For instance, in the game League of Legends, the character that a player selects has a variety of implications on the strategies and play style that will be employed. While traditional agents trained in prior research tend to focus on the gameplay exclusively and leave the player selection + configuration portion out of the game (i.e. fixed by the developer), I think training an RL agent to perform this part of the game will likely require a lot more training data since, with fewer assumptions in place, the search space to be explored becomes much larger; with that said, I think such an approach might encourage even greater creativity in learning successful or novel approaches.

The main goal of this study is for me to learn more about practical reinforcement learning libraries and design patterns, along with exploring how deep RL methods can be used to explore probabilistic search spaces that have definitive actions that an agent can take, clear success metrics, and randomness / latent variables that an agent might either ignore or learn to predict on its own. I think the main challenge of such a project would be my lack of familiarty in this space along with limitations in computing power. To facilitate this project, my plan is to start with studying and creating agents to solve very simple games (i.e. Google Chrome offline dinosaur game, Flappy Bird). The loop for this would most likely look like
1. Recreating the game in Python
2. Preparing the game environment such that it can be studied by an RL agent
3. Training an RL agent and in the process, finding a suitable policy
4. Evaluating the agent's performance.

My ambitious goal for this project would be to re-create the solutions for 2-3 games that have already been solved by RL agents from prior research. Since I am starting from a background of absolutely no prior background in RL code implementations, I think these efforts would be necessary to ramping my skill sett up to the degree of being able to do novel RL work. In addition, in the process of reimplementation, I will think about how to extend prior work on such games. My milestone report will likely be a discussion of how these implementations have faired. During the second half of the semeseter, I hope to study how I might apply such techniques to a game that hasn't been explored yet in RL research. I think the main contribution of my report will be a self-study of the nuances and efficacy of RL techniques across a variety of games that vary in their rules, available moves, and success metrics.